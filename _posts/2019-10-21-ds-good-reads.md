---
layout: post
title: "DS Good Reads: Edition 1"
date: 2019-10-21
---

## Opinions and Explanations
[Things you’re probably not using in Python 3](https://datawhatnow.com/things-you-are-probably-not-using-in-python-3-but-should/?): An old article but it's worth checking this out, and see if you can take take advantage of Python3.

How to quickly and effectively read other people’s code: I've spent the better half of the sprint going through NetOpt code and found this article useful. While most things are obvious, this puts a system/ process in place to improve your pace. If you are already good at it, you might want to check the article to figure out what 'chicken sexing' is.


## Competition Corner
Kannada Numericals MNIST: Giving this a light competition a shot. Anyone interested to collaboration are welcome.

## CodeNetSearch Challenge: This is one of a kind challenge for semantic search. The idea is to find methods from search string. The dataset is something that doesn't come by often. Six million methods overall, two million of which have associated documentation (docstrings, JavaDoc, and more) and metadata that indicates the original location. If you are in NLP/ NLU space, this is almost as hard as it can get. 


## What's New
VSCode now supports Jupyter: VSCode users rejoice..! It now supports Jupyter hub type execution(Run in cells) and plot support. 

## TensorFlow is dead, long live TensorFlow ! : The much awaited TensorFlow 2.0's stable version is released. TensorFlow has just gone full Keras, to say the least. This article by Google's very own Cassie Kozyrkov, explains the basic changes and can find tutorials on how to use TF2.0. May be it's time to switch to TensorFlow? 

## Transformers Unleashed: New version of Transformers is released, now has over 8 architectures with over 30 pretrained models, some in more than 100 languages. Biggest change? Deep interoperability between TensorFlow 2.0 and PyTorch models. See the models in action, built with transformers, autofill text brilliantly.


## SpotLight
Walk Down the networks lane: This CuratedPapers article has all the path breaking papers in deep learning by fathers of deep learning and many more. This covers critical ideas which we write with a single line of code such as Normalisation, DropOuts, ResNet50 etc. One more paper they should add to the list is word embeddings.
